---
layout:     post
title:      交叉熵损失函数说明
date:       2022-07-15
author:     (cskywit)
header-img: img/the-first.png
catalog:   true
tags:
    - 计算机视觉
---
# 交叉熵损失函数说明
## 1 交叉熵的数学原理

说起交叉熵损失函数【Cross Entropy Loss】，脑海中立马浮现出它的公式：

$L = - [ y \log y + ( 1 - y ) \log ( 1 - y ) ]$

我们已经对这个交叉熵函数非常熟悉，大多数情况下都是直接拿来使用就好。但是它是怎么来的？为什么它能表征真实样本标签和预测概率之间的差值？上面的交叉熵函数是否有其它变种？也许很多同学还不是很清楚！没关系，接下来我将尽可能以最通俗的语言回答上面这几个问题。

我们知道，在二分类问题模型：例如逻辑回归「Logistic Regression」、神经网络「Neural Network」等，真实样本的标签为 [0，1]，分别表示负类和正类。模型的最后通常会经过一个 Sigmoid 函数，输出一个概率值，这个概率值反映了预测为正类的可能性：概率越大，可能性越大。Sigmoid 函数的表达式和图形如下所示：

<img src="https://notebookimage.oss-cn-chengdu.aliyuncs.com/img/image-20220702085430389.png" alt="image-20220702085430389" style="zoom:50%;" />

其中 s 是模型上一层的输出，Sigmoid 函数有这样的特点：s = 0 时，g(s) = 0.5；s >> 0 时， g ≈ 1，s << 0 时，g ≈ 0。显然，g(s) 将前一级的线性输出映射到 [0，1] 之间的数值概率上。这里的 g(s) 就是交叉熵公式中的模型预测输出 。

我们说了，预测输出即 Sigmoid 函数的输出表征了当前样本标签为 1 的概率：

$\hat{y}=P(y=1 \mid x)$

很明显，当前样本标签为 0 的概率就可以表达成：

$1-\hat{y}=P(y=0 \mid x)$

重点来了，如果我们从极大似然性的角度出发，把上面两种情况整合到一起：

$P(y \mid x)=\hat{y}^{y} \cdot(1-\hat{y})^{1-y}$

不懂极大似然估计也没关系。我们可以这么来看：

当真实样本标签 y = 0 时，上面式子第一项就为 1，概率等式转化为：

$P(y=0 \mid x)=1-\hat{y}$

当真实样本标签 y = 1 时，上面式子第二项就为 1，概率等式转化为：

$P(y=1 \mid x)=\hat{y}$

两种情况下概率表达式跟之前的完全一致，只不过我们把两种情况整合在一起了。

重点看一下整合之后的概率表达式，我们希望的是概率 P(y|x) 越大越好。首先，我们对 P(y|x) 引入 log 函数，因为 log 运算并不会影响函数本身的单调性。则有：

$\log P(y \mid x)=\log \left(\hat{y}^{y} \cdot(1-\hat{y})^{1-y}\right)=y \log \hat{y}+(1-y) \log (1-\hat{y})$

我们希望 log P(y|x) 越大越好，反过来，只要 log P(y|x) 的负值 -log P(y|x) 越小就行了。那我们就可以引入损失函数，且令 Loss = -log P(y|x)即可。则得到损失函数为：

$L=-[y \log \hat{y}+(1-y) \log (1-\hat{y})]$

非常简单，我们已经推导出了单个样本的损失函数，是如果是计算 N 个样本的总的损失函数，只要将 N 个 Loss 叠加起来就可以了：

$L=\sum_{i=1}^{N} y^{(i)} \log \hat{y}^{(i)}+\left(1-y^{(i)}\right) \log \left(1-\hat{y}^{(i)}\right)$

这样，我们已经完整地实现了交叉熵损失函数的推导过程。

总结一下：什么是交叉熵？

**交叉熵主要是用来判定实际的输出与期望的输出的接近程度**，为什么这么说呢，举个例子：在做分类的训练的时候，如果一个样本属于第K类，那么这个类别所对应的的输出节点的输出值应该为1，而其他节点的输出都为0，即[0,0,1,0,….0,0]，这个数组也就是样本的Label，是神经网络最期望的输出结果。也就是说用它来衡量网络的输出与标签的差异，利用这种差异经过反向传播去更新网络参数。

##  2 信息量与熵

**信息量：**它是用来衡量一个事件的不确定性的；一个事件发生的概率越大，不确定性越小，则它所携带的信息量就越小。假设X是一个离散型随机变量，其取值集合为X，概率分布函数为$p(x)=P(X=x), x \in X$ ，我们定义事件 $X=x_{0}$的信息量为：  $I(x_0)=-\log \left(p\left(x_{0}\right)\right)$ ，$当p\left(x_{0}\right)=1$ 时，熵将等于0，也就是说该事件的发生不会导致任何信息量的增加。

**熵：**它是用来衡量一个系统的混乱程度的，代表一个系统中信息量的总和；信息量总和越大，表明这个系统不确定性就越大。

    举个例子：假如小明和小王去打靶，那么打靶结果其实是一个0-1分布，X的取值有{0：打中，1：打不中}。在打靶之前我们知道小明和小王打中的先验概率为10%，99.9%。根据上面的信息量的介绍，我们可以分别得到小明和小王打靶打中的信息量。但是如果我们想进一步度量小明打靶结果的不确定度，这就需要用到熵的概念了。那么如何度量呢，那就要采用**期望**了。我们对所有可能事件所带来的信息量求期望，其结果就能衡量小明打靶的不确定度：

$H_{A}(x)=-\left[p\left(x_{A}\right) \log \left(p\left(x_{A}\right)\right)+\left(1-p\left(x_{A}\right)\right) \log \left(1-p\left(x_{A}\right)\right)\right]=0.4690$

与之对应的，小王的熵（打靶的不确定度）为：

$H_{B}(x)=-\left[p\left(x_{B}\right) \log \left(p\left(x_{B}\right)\right)+\left(1-p\left(x_{B}\right)\right) \log \left(1-p\left(x_{B}\right)\right)\right]=0.0114$

    虽然小明打靶结果的不确定度较低，毕竟十次有9次都脱靶；但是小王打靶结果的不确定度更低，1000次射击只有1次脱靶，结果相当的确定。

**交叉熵：**它主要刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。假设概率分布p为期望输出，概率分布q为实际输出， $H(p, q)$ 为交叉熵，则

$H(p, q)=-\sum_{x}(p(x) \log q(x)+(1-p(x)) \log (1-q(x)))$

    那么该公式如何表示，举个例子，假设N=3，期望输出为 ，实际输出  $q1 = (0.5,0.2,0.3)$ ， $q1 = (0.8,0.1,0.1)$ ，那么： $\left.H\left(p, q_{1}\right)=-(1 \log 0.5+0 \log 0.2+0 \log 0.3+0 \log 0.5+1 \log 0.8+1 \log 0.7)\right)=0.55$  $\left.H\left(p, q_{2}\right)=-(1 \log 0.8+0 \log 0.1+0 \log 0.1+0 \log 0.2+1 \log 0.9+1 \log 0.9)\right)=0.19$

通过上面可以看出，q2与p更为接近，它的交叉熵也更小。

## 3 Pytorch中的交叉熵损失函数CrossEntropyLoss()

在使用pytorch深度学习框架，计算损失函数的时候经常会遇到这么一个函数：

$nn.CrossEntropyLoss()$

该损失函数结合了$nn.LogSoftmax()$和$nn.NLLLoss()$两个函数。它在做分类（具体几类）训练的时候是非常有用的。在训练过程中，对于每个类分配权值，可选的参数权值应该是一个1D张量。当你有一个不平衡的训练集时，这是是非常有用的。那么针对这个函数，下面将做详细的介绍。

Pytorch中计算的交叉熵并不是采用$H(p, q)=-\sum_{x}(p(x) \log q(x)+(1-p(x)) \log (1-q(x)))$

而是交叉熵的另外一种方式计算得到的：$H(p, q)=-\sum_{x}(p(x) \log q(x)$

它是交叉熵的另外一种方式。

Pytorch中$CrossEntropyLoss()$函数的主要是将$softmax-log-NLLLoss$合并到一块得到的结果。

1.$Softmax$后的数值都在0~1之间，所以$ln$之后值域是负无穷到0。

2.然后将$Softmax$之后的结果取$log$，将乘法改成加法减少计算量，同时保障函数的单调性 。

3.$NLLLoss$的结果就是把上面的输出与$Label$对应的那个值拿出来(下面例子中就是：将log_output\logsoftmax_output中与y_target对应的值拿出来，去掉负号，再求均值。 下面是我仿真写的一个例子：

```python 
import torch
import torch.nn as nn
x_input=torch.randn(3,3)#随机生成输入 
print('x_input:\n',x_input) 
y_target=torch.tensor([1,2,0])
#设置输出具体值 
print('y_target\n',y_target)

x_input:
 tensor([[-0.5215, -0.4240,  0.8907],
        [-0.2820,  0.0445,  1.3734],
        [-1.0393,  0.8725,  0.1539]])
y_target
 tensor([1, 2, 0])
*******************************************************************    
#计算输入softmax，此时可以看到每一行加到一起结果都是1
softmax_func=nn.Softmax(dim=1)
soft_output=softmax_func(x_input)
print('soft_output:\n',soft_output)

soft_output:
 tensor([[0.1611, 0.1776, 0.6613],
        [0.1312, 0.1819, 0.6869],
        [0.0904, 0.6115, 0.2981]])

******************************************************************* 

#在softmax的基础上取log
log_output=torch.log(soft_output)
print('log_output:\n',log_output)

#对比softmax与log的结合与nn.LogSoftmaxloss(负对数似然损失)的输出结果，发现两者是一致的。
logsoftmax_func=nn.LogSoftmax(dim=1)
logsoftmax_output=logsoftmax_func(x_input)
print('logsoftmax_output:\n',logsoftmax_output)

logsoftmax_output:
 tensor([[-1.8258, -1.7282, -0.4135],
        [-2.0309, -1.7044, -0.3756],
        [-2.4036, -0.4918, -1.2104]])


******************************************************************* 
#pytorch中关于NLLLoss的默认参数配置为：reducetion=True、size_average=True
nllloss_func=nn.NLLLoss()
nlloss_output=nllloss_func(logsoftmax_output,y_target)
print('nlloss_output:\n',nlloss_output)

nlloss_output:
 tensor(1.5025)
******************************************************************* 

#直接使用pytorch中的loss_func=nn.CrossEntropyLoss()看与经过NLLLoss的计算是不是一样
crossentropyloss=nn.CrossEntropyLoss()
crossentropyloss_output=crossentropyloss(x_input,y_target)
print('crossentropyloss_output:\n',crossentropyloss_output)

crossentropyloss_output:
 tensor(1.5025)
```

